---
title: "Prediction Assignment"
author: "KKher"
date: "8/30/2020"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, error = FALSE, warning = FALSE)
```

## Objective

The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. You may use any of the other variables to predict with.

In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website [here](http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har) (see the section on the Weight Lifting Exercise Dataset).

## Data

The data for this project come from this [source](http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har)
- The training data for this project are available [here](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)
- The test data are available [here](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)

```{r data_lib}
# import libraries
library(plotly);library(caret);

# import data
train <- read.csv("pml-training.csv")
test <- read.csv("pml-testing.csv")
```

## Data Wrangling

First we need to clean up our dataset and only use column with data.
- Remove unwanted columns
- Handle NAs
```{r cleanup}
# removing columns with NAs
train <- train[, colSums(is.na(train)) == 0]

NZV <- nearZeroVar(train)
train <- train[, -NZV]

# removing columns with unneeded data
train <- train[, -c(1:7)]

# examine our objective column
plot_ly(x = train$classe, type = "histogram", histnorm = "probability") %>%
  layout(title = "Propotion of Classe variable in Train dataset",
         xaxis= list(title = "Classe"), yaxis= list(title = "Probability") )
```

## Prediction Technique

Two main techniques will be Random Forest & MultiNomial modeling.

- First Model: Decision Tree Classification
``` {r algorithm1}
library(rpart)
library(rpart.plot)

# create training, validation
inTrain <- createDataPartition(train$classe, p = 0.7, list = FALSE)
training <- train[inTrain, ]
validation <- train[-inTrain, ]

training$classe <- as.factor(training$classe)
validation$classe <- as.factor(validation$classe)

# build model & plot result
mod1 <- rpart(classe ~ ., data = training, method = "class",na.action = na.pass)
rpart.plot(mod1, main="Model#1: Decision Tree")

# use model#1 to calculate prediction
pred1 <- predict(mod1, newdata=validation, type = "class")
confusionTree <- confusionMatrix(pred1, validation$classe)
confusionTree
```

- Second Model: Random Forest
Use number of trees = 50 with 5 main features per tree

```{r algorithm2}
library(randomForest)

# build model & plot result .. 
mod2 <- randomForest(classe ~ ., data=training, ntree=50, mtry=5, importance=TRUE)
plot(mod2, log="y")

# use model#2 to calculate prediction
pred2 <- predict(mod2, validation)
confusionRF <- confusionMatrix(pred2, validation$classe)
confusionRF
```


- Use Random Forest Model to predict Classe from test data, as its accuracy (0.98) is greater than decision tree model (0.70)
```{r Test_Results}
predict(mod2, newdata=test)
```